[debug] Created tunnel using local port: '43167'

[debug] SERVER: "127.0.0.1:43167"

[debug] Original chart version: ""
[debug] CHART PATH: /home/matt/go/src/k8s.io/charts/stable/linkerd

NAME:   callous-pug
REVISION: 1
RELEASED: Sat Jun  2 15:00:24 2018
CHART: linkerd-0.4.1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
config: null
kubectl:
  image: buoyantio/kubectl:v1.6.2
  resources:
    requests:
      cpu: 0
      memory: 32Mi
linkerd:
  discoveryPortName: http
  httpPort: 80
  image: buoyantio/linkerd:1.1.2
  ingress:
    enabled: false
  name: l5d
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 0
      memory: 512Mi
prometheus:
  path: /admin/metrics/prometheus
  probe: true
  scrape: true
service:
  type: ClusterIP

HOOKS:
MANIFEST:

---
# Source: linkerd/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: callous-pug-linkerd
    chart: "linkerd-0.4.1"
    heritage: "Tiller"
    release: "callous-pug"
  name: callous-pug-linkerd-config
data:

  config.yaml: |-
    admin:
      port: 9990

    namers:
    - kind: io.l5d.k8s
      # kubectl proxy forwards localhost:8001 to the Kubernetes master API
      host: localhost
      port: 8001
    routers:
    - protocol: http
      # Incoming requests to linkerd with a Host header of "hello" get assigned
      # a name like /http/1.1/GET/hello.  This dtab transforms that into
      # /#/io.l5d.k8s/default/http/hello which indicates that the kubernetes
      # namer should query the API for ports named "http" on pods in the
      # "default" namespace named "hello".  linkerd will then load balance over
      # those pods.
      dtab: |
        /http/1.1/* => /#/io.l5d.k8s/default/http
      servers:
      - ip: 0.0.0.0
        port: 4140
---
# Source: linkerd/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: callous-pug-linkerd
  labels:
    chart: "linkerd-0.4.1"
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/probe: "true"
    prometheus.io/path: "/admin/metrics/prometheus"
    prometheus.io/port: "9990"
spec:
  type: ClusterIP
  ports:
  - name: outgoing
    port: 4140
  - name: incoming
    port: 4141
  - name: admin
    port: 9990
  selector:
    app: callous-pug-linkerd
---
# Source: linkerd/templates/daemonset.yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  labels:
    app: callous-pug-linkerd
    chart: "linkerd-0.4.1"
    heritage: "Tiller"
    release: "callous-pug"
  name: callous-pug-linkerd
spec:
  template:
    metadata:
      annotations:
      labels:
        app: callous-pug-linkerd
        release: "callous-pug"
    spec:
      volumes:
      - name: callous-pug-linkerd-config
        configMap:
          name: "callous-pug-linkerd-config"
      containers:
      - name: l5d
        image: buoyantio/linkerd:1.1.2
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        args:
        - /io.buoyant/linkerd/config/config.yaml
        ports:
        - name: incoming
          containerPort: 4141
          hostPort: 4141
        - name: outgoing
          containerPort: 4140
          hostPort: 4140
        - name: admin
          containerPort: 9990
          hostPort: 9990
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 0
            memory: 512Mi
          
        livenessProbe:
          httpGet:
            path: /admin
            port: 9990
          initialDelaySeconds: 10
          periodSeconds: 5
        volumeMounts:
        - name: "callous-pug-linkerd-config"
          mountPath: "/io.buoyant/linkerd/config"
          readOnly: true
      - name: kubectl
        image: buoyantio/kubectl:v1.6.2
        args:
        - "proxy"
        - "-p"
        - "8001"
        resources:
          requests:
            cpu: 0
            memory: 32Mi
